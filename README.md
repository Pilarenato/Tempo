# Tempo
Previsão de Tempo de Trabalho Hadoop  Código utilizado para realizar algumas experiências de predicções de trabalho do Hadoop usando o OpenStack Sahara.  Detalhes do tamanho do cluster  O mestre e os escravos dos clusters tinham um sabor com as seguintes configurações:  2 VCPUs 45 GB de HD 4 GB de RAM Troca de 4 GB Executando a experiência  Para executar esta experiência primeiro você deve:  Obter a entrada Compile classes na pasta de origem e obtenha o arquivo jar. Ou obtenha o arquivo jar disponível neste repositório. Coloque o que é necessário no Sara Crie um arquivo de configuração do Json com a mesma estrutura de "configuration_default.json" Deixe este experimento impressionante correr e ir ter algum bom tempo, ele irá enviá-lo quando ele é feito;) Gerar gráficos para obter um resultado mais visual 1. Obtendo o arquivo de entrada  O arquivo de entrada usado no experimento pode ser acessado neste link .  Este arquivo foi gerado por TeraSortGen de Hadoop 1.2.1 exemplos, fazendo o seguinte comando:  bin/hadoop jar hadoop-examples-1.2.1.jar terasortgen 50000000  Se você quiser gerar o arquivo você mesmo, você deve:  Instale o hadoop 1.2.1 Descompacte o Hadoop baixado. Edite o arquivo conf / hadoop-env.sh para definir pelo menos JAVA_HOME como a raiz da sua instalação Java. Então corra : bin/hadoop jar hadoop-examples-1.2.1.jar terasortgen 50000000 Em caso de dúvidas: mais informações sobre o TeraSortGen aqui 2. Compilar as classes e gerar o arquivo JAR  Para compilar as classes você deve:  Tenha hadoop 2.6.0 instalado, você pode ter mais informações sobre como fazê-lo aqui Baixe a pasta de origem e coloque-a no mesmo diretório que o hadoop está instalado (você pode colocar a pasta de origem em outro lugar, mas torna mais fácil se tudo estiver no mesmo lugar e você pode excluí-la quando terminar se você Não quero isso na pasta hadoop) Depois de obter a pasta de origem (e está na mesma pasta do hadoop), compile as classes com os comandos thess:  $export JAVA_HOME=/usr/java/default $export PATH=${JAVA_HOME}/bin:${PATH} $export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar  $ bin/hadoop com.sun.tools.javac.Main source/*.java Agora crie o frasco em execução:   $jar cf experiment.jar source/*.class  Se você tiver dúvidas sobre os tópicos 2 e 3, você pode ter mais informações sobre ele aqui .  3. Colocar o que é necessário no Sara  Crie um key_pair, se você já tiver um que você pode usá-lo (O caminho local de sua chave pública e privada será necessária) Você terá que colocar o frasco como binário de trabalho no Sahara e criar um modelo de trabalho do tipo JavaAction para cada trabalho (PiEstimator, TeraSort e WordCount). Você pode ter acesso a um processo similar aqui . Você terá que criar um modelo de grupo de nó mestre e woker e um modelo de cluster com nós (3,4,5, ... 10). Um processo semelhante pode ser visto aqui . Criar um volume e colocar o arquivo de 5GB nele. Você pode entrar em contato comigo se você precisar de ajuda neste processo, eu pretendo fazer algum post sobre isso, e quando eu fizer vou colocar aqui! 4. Altere o arquivo de configuração do json  Você pode obter todas essas informações através do Horizon, exceto: public_keypair_path, private_keypair_path e private_keypair_name que somente você tem acesso.  5. Corra, baby, corra!  Agora tudo deve estar pronto para correr :coração:! Você pode executar esta experiência de duas maneiras diferentes:  Correndo $python runExperiment.py &lt;number of executions> &lt;configuration path> &lt;output file name> Com número de execuções = 8 Running $python runExperimentIndividuall.py &lt;number of executions> &lt;number of cluster nodes> &lt;configuration path> &lt;output file name> Com número de execuções = 8 e número de cluster = [3,10] 6. Gerar gráficos  Agora que você tem os arquivos ouput, a etapa final é gerar os gráficos. Se você usou runExperimentIndividually.py você deve concatenar todos os arquivos em um, você pode fazer isso por: $ cat &lt;output_1_node> &lt;output_2_nodes> &lt;output_3_nodes> ... > output_exp  ATENTION: antes de executar scripts, altere os nomes input_file e output_file. Verifique também se os arquivos estão na mesma pasta ou altere o caminho no começo do script com o comando: setwd("your_path")  Em seguida, vá para a pasta de análise e faça o seguinte:  Executar o filtrado. Ele irá gerar um novo arquivo nomeado como output_name. Execute KNN.R no arquivo gerado anteriormente e ele gerará um novo arquivo. Execute graphs_cost.R e graphs_prediction.R com a entrada = saída KNN.R. Eles vão gerar gráficos em formato pdf.  E agora você tem alguns gráficos impressionantes :oculos de sol: !!!
